{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b312cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data handling and trasformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualisation \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "# machine learning metrics and scoring\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# machine learning models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d1155f3",
   "metadata": {},
   "source": [
    "### Put presets in place"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823d3bc3",
   "metadata": {},
   "source": [
    "#### Colorblind-friendly palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with the colors you want to use\n",
    "colors = [\"#FF0B04\", \"#4374B3\",\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"]\n",
    "# Set a custom color palette for colour blind readers\n",
    "\n",
    "# Assign a name to the palette\n",
    "colorblind1 = sns.set_palette(sns.color_palette(colors)) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38331648",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "486e1df4",
   "metadata": {},
   "source": [
    "During the EDA phase, we cleaned the data and separated the dataset into two separate dataframes:\n",
    "\n",
    "X = the independent variables\n",
    "\n",
    "y = the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'./../data/X.csv') # import the csv file using a relative path to the 'data' folder\n",
    "\n",
    "#X= X.drop(columns=['Unnamed: 0']) # drop index column\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7530a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(r'./../data/y.csv') # import the csv file using a relative path to the 'data' folder\n",
    "#y= y.drop(columns=['Unnamed: 0']) # drop index column\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cats = y.copy() # make a separate df to preserve the categorical variables\n",
    "y_cats.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce9e8d70",
   "metadata": {},
   "source": [
    "### Examine the class label imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253238c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count instances of the target variable \n",
    "#count frequencies\n",
    "counts = y['offer_accepted'].value_counts()\n",
    "##count and compute as percentage of total \n",
    "pct = y['offer_accepted'].value_counts(normalize=True)\n",
    "\n",
    "#concatenate results into one DataFrame\n",
    "pd.concat([counts,pct], axis=1, keys=['n', '%'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d14789c2",
   "metadata": {},
   "source": [
    "Convert the 'Yes' and 'no' values to '1' and '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be72150",
   "metadata": {},
   "outputs": [],
   "source": [
    "y['offer_accepted'] = y['offer_accepted'].replace({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7331f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(y['offer_accepted'])\n",
    "total = neg + pos\n",
    "\n",
    "print('Examples:\\n    Total: {}\\n    Customers who accepted credit card offer: {} ({:.2f}% of total customer base)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4881f46",
   "metadata": {},
   "source": [
    "Before applying our machine learning model to improve the balance between 'Yes' and 'No', we need to scale the numerical variables. \n",
    "\n",
    "We'll be using standard scaler to remove the mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "independents = X.copy() # make a separate df to preserve the column headers\n",
    "\n",
    "transformer = StandardScaler().fit(independents)\n",
    "standard_x = transformer.transform(independents)\n",
    "X = pd.DataFrame(standard_x)\n",
    "X.columns = independents.columns # put the column headers back\n",
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9504266",
   "metadata": {},
   "source": [
    "#### Optional:\n",
    "\n",
    "We can plot the data before and after to see if there are any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(independents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca645b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorblind1 = sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "#Create a matrix of panels sharing the same y axis (total claim amount)\n",
    "\n",
    "fig, axs = plt.subplots(10, 2, figsize=(30, 50)) # width and height \n",
    "axs = axs.flatten()\n",
    "fig.subplots_adjust(hspace=.2, wspace=.2) # ensure that the plots do not overlap\n",
    "\n",
    "\n",
    "for i, column in enumerate(X.columns):\n",
    "    sns.histplot(data=X, x=column, palette=sns.color_palette(\"colorblind\"), bins=25, element='step', stat='density', alpha=0.7, ax=axs[i]) # use element='step' to create a histogram-like plot, and stat='density' to normalize the y-axis\n",
    "    axs[i].set_title(column)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ef0b465",
   "metadata": {},
   "source": [
    "In the bar plot below, we see that the majority class, 'No' is much larger than the minority class 'Yes'. \n",
    "\n",
    "A problem with using these data with such an imbalance is that if we attempt to predict whether or not a client will accept an offer of a credit card (i.e., a 'yes' or 'no' answer), then our machine learning models risk being biased towards the majority class. \n",
    "\n",
    "They will tend to less accurate when predicting 'Yes' and over-predict 'No's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646094b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_cats[\"offer_accepted\"], palette=sns.color_palette(\"colorblind\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ad0a76c",
   "metadata": {},
   "source": [
    "## Training the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfb31be4",
   "metadata": {},
   "source": [
    "**test_size** =  proportion of the data that should be used for testing. In this case, 30% of the data will be used for testing, while the remaining 70% will be used for training.\n",
    "\n",
    "**random_state** = is a parameter that sets the random seed, which ensures that the same random split is generated each time the code is run. This is useful for reproducibility.\n",
    "\n",
    "The function returns four objects:\n",
    "\n",
    "#### Indepedent variables\n",
    "\n",
    "*X_train* is a subset of the input dataset containing the independent variables for the training set.\n",
    "\n",
    "\n",
    "*X_test* is a subset of the input dataset containing the independent variables for the testing set.\n",
    "\n",
    "#### Dependent variables\n",
    "\n",
    "*y_train* is a subset of the target variable corresponding to the observations in X_train.\n",
    "\n",
    "*y_test* is a subset of the target variable corresponding to the observations in X_test.\n",
    "\n",
    "By splitting the dataset into training and testing sets, we can train a machine learning model on the training set and evaluate its performance on the testing set. This helps to assess the generalization ability of the model, i.e. how well it can predict on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429dfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# describes info about train and test set\n",
    "\n",
    "print(\"Number CC acceptance X_train dataset: \", X_train.shape)\n",
    "print(\"Number CC acceptance y_train dataset: \", y_train.shape)\n",
    "print(\"Number CC acceptance X_test dataset: \", X_test.shape)\n",
    "print(\"Number CC acceptance y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5150c7f",
   "metadata": {},
   "source": [
    "Then we train a logistic regression classification model on the training data and then use the model to make predictions on new test data.\n",
    "\n",
    "The predicted labels can then be compared to the true labels (y_test) to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classing = LogisticRegression(random_state=0, multi_class = 'ovr').fit(X_train, y_train)\n",
    "predictions = classing.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ddcb4e",
   "metadata": {},
   "source": [
    "We can make a confusion matrix to see how it predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "240fe63a",
   "metadata": {},
   "source": [
    "and graph the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f98233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35278617",
   "metadata": {},
   "source": [
    "<blockquote style=\"background-color: #F1F8E9; color: #37474F; border-color: #558B2F; padding: 10px; border-radius: 5px;\">\n",
    "            \n",
    "**Interpreting the confusion matrix:**\n",
    "    \n",
    "_____________\n",
    "\n",
    "From the above confusion matrix, we can see that we were able to predict:\n",
    "\n",
    "- **3405 True Negatives (Top-Left Quadrant)**\n",
    "- 0 False Positives (Top-Right Quadrant)\n",
    "- 191 False Negatives (Bottom-Left Quadrant)\n",
    "- 0 True Positives (Bottom-Right Quadrant)\n",
    "\n",
    "This suggests that our model is heavily biased towards true negatives or the 'No', i.e, rejecting the credit card.\n",
    "_____________\n",
    "</blockquote>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98894fac",
   "metadata": {},
   "source": [
    "When we run our accuracy score function to assess our logistic regression, we see that the R2 is very high:\n",
    "\n",
    "0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45989bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_score = classing.score(X_test,y_test)\n",
    "\n",
    "print(\"Accuracy score:\", classification_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc161e00",
   "metadata": {},
   "source": [
    "<blockquote style=\"background-color: #E3F2FD; color: #37474F; border-color: #1565C0; padding: 10px; border-radius: 5px;\">\n",
    "            \n",
    "##  ðŸ“Š Evaluating our data : what do the scores mean? ðŸ“Š\n",
    "    \n",
    "_____________\n",
    "\n",
    "\n",
    "**Accuracy:** This is the most common metric for evaluating the performance of a classification model. It measures the proportion of correctly classified instances (both true positives and true negatives) out of all instances in the dataset.\n",
    "\n",
    "**Precision:** Precision measures the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive (both true positives and false positives). It is a measure of the model's ability to correctly identify positive instances.\n",
    "\n",
    "**Recall:** Recall (also known as sensitivity) measures the proportion of correctly predicted positive instances (true positives) out of all actual positive instances (both true positives and false negatives). It is a measure of the model's ability to identify all positive instances.\n",
    "\n",
    "**F1-score:** F1-score is a harmonic mean of precision and recall, and provides a balanced measure between the two. It ranges between 0 and 1, with higher values indicating better performance. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "**AUC-ROC:** AUC-ROC (Area Under the Receiver Operating Characteristic Curve) is a metric that measures the ability of a binary classification model to distinguish between positive and negative instances. It is calculated by plotting the true positive rate (recall) against the false positive rate (1 - specificity) at different classification thresholds, and calculating the area under the resulting curve. The AUC-ROC ranges from 0 to 1, with higher values indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108281aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "805a1261",
   "metadata": {},
   "source": [
    "If we run an f-1 score we can see that it is very similar to our accuracy score:\n",
    "\n",
    "| precision | recall | f1-score | support |\n",
    "| --------- | ------ | -------- | ------- |\n",
    "| 0.95      | 1      | 0.97     | 3405    |\n",
    "| 0         | 0      | 0        | 191     |\n",
    "| accuracy  |        | 0.95     | 3596    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82484a90",
   "metadata": {},
   "source": [
    "## Resampling the data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3b59edd",
   "metadata": {},
   "source": [
    "To balance the data, we'll try oversampling using the Synthetic Minority Over-sampling Technique (SMOTE). This will balance the class distribution of the target variable 'offer_accepted'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5347ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X = independents[['bank_accounts_open',\n",
    " 'credit_cards_held',\n",
    " 'homes_owned',\n",
    " 'household_size',\n",
    " 'average_balance',\n",
    " 'q1_balance',\n",
    " 'q2_balance',\n",
    " 'q3_balance',\n",
    " 'q4_balance',\n",
    " 'income_level',\n",
    " 'credit_rating',\n",
    " 'reward_Air Miles',\n",
    " 'reward_Cash Back',\n",
    " 'reward_Points',\n",
    " 'mailer_type_Letter',\n",
    " 'mailer_type_Postcard',\n",
    " 'overdraft_protection_No',\n",
    " 'overdraft_protection_Yes',\n",
    " 'own_your_home_No',\n",
    " 'own_your_home_Yes']]\n",
    "transformer = StandardScaler().fit(X)\n",
    "X = transformer.transform(X)\n",
    "y = y_cats['offer_accepted']\n",
    "X_sm, y_sm = smote.fit_resample(X, y)\n",
    "y_sm.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ce7e56",
   "metadata": {},
   "source": [
    "The classes have been balanced using SMOTE, so you can see that the number of samples with label 'Yes' is equal to the number of samples with label 'No'. This means that the class distribution is now even, with both classes having the same number of samples (in this case, 16955)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed2dbc5",
   "metadata": {},
   "source": [
    "Now our model has a lower classification score:\n",
    "\n",
    "From 0.95 to 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd811723",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=100)\n",
    "classification = LogisticRegression(random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "predictions = classification.predict(X_test)\n",
    "classification_score = classification.score(X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f17e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7043153445394672\n",
      "Precision: 0.7060225290188572\n",
      "Recall: 0.7043153445394672\n",
      "F1-score: 0.7039406685141181\n",
      "AUC-ROC: 0.7750509332032841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=100)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using various metrics\n",
    "classification_score = classification.score(X_test, y_test) # score from scikit learn is identical to accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted') #  give more importance to the performance on the positive class\n",
    "recall = recall_score(y_test, y_pred, average='weighted') # give more importance to the performance on the positive class\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') # give more importance to the performance on the positive class\n",
    "auc_roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"AUC-ROC:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaad935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the ROC-AUC curve\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af627c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# predict probabilities of positive class\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calculate AUC-ROC score\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC={auc_score:.3f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18c1e8",
   "metadata": {},
   "source": [
    "### UnderSampling using TomekLinks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb92f032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offer_accepted\n",
       "No                0.941674\n",
       "Yes               0.058326\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl = tl.fit_resample(X, y_cats)\n",
    "y_tl.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a689ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9364051789794364\n",
      "Precision: 0.8768546592195103\n",
      "Recall: 0.9364051789794364\n",
      "F1-score: 0.9056520492076438\n",
      "AUC-ROC: 0.745026214346419\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tl, y_tl, test_size=0.3, random_state=100)\n",
    "classification = LogisticRegression(random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = classification.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using various metrics\n",
    "classification.score(X_test, y_test)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Evaluate the performance using various metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted') #  give more importance to the performance on the positive class\n",
    "recall = recall_score(y_test, y_pred, average='weighted') # give more importance to the performance on the positive class\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') # give more importance to the performance on the positive class\n",
    "auc_roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"AUC-ROC:\", auc_roc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f4dc8ab",
   "metadata": {},
   "source": [
    "The AUC-ROC score may be lower than the other metrics if the model is making many false positive predictions, which would be penalized more heavily when the decision threshold is set higher.\n",
    "\n",
    "However, we've seen that our model makes more false-negative predictions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbcb9cc4",
   "metadata": {},
   "source": [
    "<blockquote style=\"background-color: #F1F8E9; color: #37474F; border-color: #558B2F; padding: 10px; border-radius: 5px;\">\n",
    "        \n",
    "_____________\n",
    "\n",
    "## ðŸš§ analysis & next steps ðŸš§ \n",
    "\n",
    "Learning towards towards the **SMOTE** for now as:\n",
    "- by oversampling, the minority class is more represented and it's consistently in the .70s\n",
    "\n",
    "**Tomek links**\n",
    "- the AUC-ROC score in tomek links is lower than in the SMOTE (lower = less accurate)\n",
    "- the accuracy, recall, f1 in tomek links  are all in the 90s that suggests that the majority class (i.e., No) is still over represented \n",
    "\n",
    "\n",
    "\n",
    "To do:\n",
    "\n",
    "- Choose which features have the bigget effect on imbalance \n",
    "- yes and not is affected by these features\n",
    "\n",
    "- run a model with the greatest predictive value \n",
    "- rule out variables that are less important\n",
    "- which scaler performed the best and why\n",
    "cf. \n",
    "sklearn feature importance  logitic regression \n",
    "\n",
    "\n",
    "\n",
    "_____________\n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b23cb79e",
   "metadata": {},
   "source": [
    "EXPORT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c133091",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create a DataFrame for the oversampled data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbank_accounts_open\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcredit_cards_held\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhomes_owned\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhousehold_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_balance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mq1_balance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mq2_balance\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mq3_balance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mq4_balance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mincome_level\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcredit_rating\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moverdraft_protection_No\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moverdraft_protection_Yes\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mown_your_home_No\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mown_your_home_Yes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m Xy_sm \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_sm, columns\u001b[39m=\u001b[39mcols)\n\u001b[1;32m     10\u001b[0m Xy_sm[\u001b[39m'\u001b[39m\u001b[39moffer_accepted\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m y_sm\n\u001b[1;32m     12\u001b[0m \u001b[39m# Export the DataFrame to a CSV file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_sm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataFrame for the oversampled data\n",
    "cols = ['bank_accounts_open', 'credit_cards_held', 'homes_owned',\n",
    "        'household_size', 'average_balance', 'q1_balance', 'q2_balance',\n",
    "        'q3_balance', 'q4_balance', 'income_level', 'credit_rating',\n",
    "        'reward_Air Miles', 'reward_Cash Back', 'reward_Points',\n",
    "        'mailer_type_Letter', 'mailer_type_Postcard',\n",
    "        'overdraft_protection_No', 'overdraft_protection_Yes',\n",
    "        'own_your_home_No', 'own_your_home_Yes']\n",
    "Xy_sm = pd.DataFrame(X_sm, columns=cols)\n",
    "Xy_sm['offer_accepted'] = y_sm\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "Xy_sm.to_csv('oversampled_data.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f41560a7",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
